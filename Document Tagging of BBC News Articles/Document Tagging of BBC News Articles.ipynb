{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This corpus includes 2,225 documents from BBC's news website corresponding to stories in five topical areas (business, entertainment, politics, sport, tech) from 2004-2005. The CSV file includes two columns: category (the five class labels) and text (pre-processed article content). In this exerceise I will use only the text column in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "#Genism\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models import LsiModel, CoherenceModel\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "\n",
    "#NLTK\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#TextBlob\n",
    "from textblob import TextBlob\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task:-\n",
    "###### -----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#### Extract the top five topics for each article that can be used for keyword search and retrieval.Using LSI/LSA and LDA algorithms, after vectorizing the text using TF-IDF vector in three different ways:\n",
    "\n",
    "#### (1) after normal cleaning of the text corpus (punctuation removal, stopword removal, etc.),\n",
    "##### (2) with term frequency filter, to exclude the top 10% of the most frequent words and words that appear less than 5 times in the documents (drawing from Zipf's Law), and\n",
    "###### (3) with a part of speech filter, to limit your TD-IDF matrix to nouns only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Loading Data\n",
    "os.chdir('D:/USF/Text Analytics/Assignment3-Topic Modelling of BBC News Articles')\n",
    "articles_df=pd.read_csv(\"BBC-articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function returns a list of words after text cleaning and list of words with only nouns.\n",
    "def text_corpus_cleaning(input_text):\n",
    "    input_text = input_text.lower()    #Converting all the words in article into lower case\n",
    "    for char in '!#$%&@?,.:;+-*/=<>\"\\'()[\\\\]X{|}~\\n\\t': #Removing all the special characters from the text inside article\n",
    "        input_text = input_text.replace(char, ' ')\n",
    "    word_list=re.findall(r\"([a-zA-z]+)\\s\",input_text) #Make a list of words present in the article.\n",
    "    #Remove all the stop words and include only the words with minimum length of 2.\n",
    "    word_list=[w for w in word_list if w not in stop_words and len(w) > 2]\n",
    "    #Lemmatization of words\n",
    "    word_list = [lemmatizer.lemmatize(w) for w in word_list]\n",
    "    modified_text=' '.join([w for w in word_list])\n",
    "    blob_object = TextBlob(modified_text)\n",
    "    #Limiting the word list with nouns\n",
    "    word_list_nouns = [word for word,pos in blob_object.tags if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS')]\n",
    "    return word_list,word_list_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=[]\n",
    "tokens_nouns=[]\n",
    "for i in range(len(articles_df['text'])):\n",
    "    tokens.append(text_corpus_cleaning(articles_df['text'][i])[0])\n",
    "    tokens_nouns.append(text_corpus_cleaning(articles_df['text'][i])[1])\n",
    "#Creating dictionary based after cleaning the data according to method1.\n",
    "myDict = corpora.Dictionary(tokens)\n",
    "dtm = [myDict.doc2bow(doc) for doc in tokens]\n",
    "tfidf_vectorizer = TfidfModel(dtm) \n",
    "tfidf = tfidf_vectorizer[dtm] \n",
    "#LSI model\n",
    "lsi_model = LsiModel(corpus=tfidf, id2word=myDict, num_topics=5)\n",
    "#Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=tfidf,\n",
    "                                       id2word=myDict,\n",
    "                                       num_topics=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To reduce dimensionality, we can filter out tokens that occur in less than  5 documents (absolute number) or more than 0.90 (fraction of total corpus size), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dictionary based after cleaning the data according to method2.\n",
    "myDict_2 = Dictionary(tokens)\n",
    "myDict_2.filter_extremes(no_below=5, no_above=0.90)\n",
    "dtm_2 = [myDict_2.doc2bow(doc) for doc in tokens]\n",
    "tfidf_vectorizer = TfidfModel(dtm_2) \n",
    "tfidf = tfidf_vectorizer[dtm_2] \n",
    "#LSI model\n",
    "lsi_model_2 = LsiModel(corpus=tfidf, id2word=myDict_2, num_topics=5)\n",
    "#Build LDA model\n",
    "lda_model_2 = gensim.models.LdaMulticore(corpus=tfidf,\n",
    "                                       id2word=myDict_2,\n",
    "                                       num_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dictionary based after cleaning the data according to method3.\n",
    "myDict_nouns=corpora.Dictionary(tokens_nouns)\n",
    "dtm_nouns=[myDict_nouns.doc2bow(doc) for doc in tokens_nouns]\n",
    "tfidf_vectorizer = TfidfModel(dtm_nouns) \n",
    "tfidf = tfidf_vectorizer[dtm_nouns] \n",
    "#LSI Model\n",
    "lsi_model_nouns = LsiModel(corpus=tfidf, id2word=myDict_nouns, num_topics=5)\n",
    "lsi_model_nouns.print_topics(num_topics=5, num_words=5)\n",
    "#LDA Model\n",
    "lda_model_nouns = gensim.models.LdaMulticore(corpus=tfidf,\n",
    "                                       id2word=myDict_nouns,\n",
    "                                       num_topics=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to return top keywords for each document based on the model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopicWords(model,corpus,n=10):\n",
    "    topic=sorted(model[corpus],key=lambda tup: -1+tup[1])[0]\n",
    "    top10=model.show_topic(topic[0],n)\n",
    "    words,_=zip(*top10)\n",
    "    return ','.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(articles_df['text'])):\n",
    "    articles_df.at[i,'Method1_LSI']=getTopicWords(lsi_model,dtm[i],n=5)\n",
    "    articles_df.at[i,'Method2_LSI']=getTopicWords(lsi_model_2,dtm_2[i],n=5)\n",
    "    articles_df.at[i,'Method3_LSI']=getTopicWords(lsi_model_nouns,dtm_nouns[i],n=5)\n",
    "    articles_df.at[i,'Method1_LDA']=getTopicWords(lda_model,dtm[i],n=5)\n",
    "    articles_df.at[i,'Method2_LDA']=getTopicWords(lda_model_2,dtm_2[i],n=5)\n",
    "    articles_df.at[i,'Method3_LDA']=getTopicWords(lda_model_nouns,dtm_nouns[i],n=5)\n",
    "    list7=articles_df['Method1_LSI'][i].split(\",\")+articles_df['Method2_LSI'][1].split(\",\")+articles_df['Method3_LSI'][i].split(\",\")+articles_df['Method1_LDA'][i].split(\",\")+articles_df['Method2_LDA'][i].split(\",\")+articles_df['Method3_LDA'][i].split(\",\")    \n",
    "    list7= [word for word, word_count in Counter(list7).most_common(5)]\n",
    "    Top5_words=','.join([w for w in list7])\n",
    "    articles_df.at[i,'Top5_words']=Top5_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/nlp/gensim-tutorial/\n",
    "\n",
    "https://usflearn.instructure.com/courses/1389096/files/85339820/download\n",
    "\n",
    "https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>Method1_LSI</th>\n",
       "      <th>Method2_LSI</th>\n",
       "      <th>Method3_LSI</th>\n",
       "      <th>Method1_LDA</th>\n",
       "      <th>Method2_LDA</th>\n",
       "      <th>Method3_LDA</th>\n",
       "      <th>Top5_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>labour,election,blair,tax,game</td>\n",
       "      <td>labour,election,game,film,blair</td>\n",
       "      <td>phone,film,economy,growth,technology</td>\n",
       "      <td>film,game,mobile,music,player</td>\n",
       "      <td>film,rate,mobile,music,game</td>\n",
       "      <td>phone,music,film,company,sale</td>\n",
       "      <td>film,game,music,labour,election</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>labour,election,blair,tax,game</td>\n",
       "      <td>labour,election,game,film,blair</td>\n",
       "      <td>phone,film,economy,growth,technology</td>\n",
       "      <td>labour,film,brown,england,party</td>\n",
       "      <td>game,mobile,technology,phone,company</td>\n",
       "      <td>film,game,election,phone,party</td>\n",
       "      <td>game,film,labour,election,phone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>labour,election,blair,tax,game</td>\n",
       "      <td>labour,election,game,film,blair</td>\n",
       "      <td>film,game,england,oscar,award</td>\n",
       "      <td>film,game,mobile,music,player</td>\n",
       "      <td>film,labour,election,party,tax</td>\n",
       "      <td>game,film,music,election,player</td>\n",
       "      <td>game,film,election,labour,blair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>mobile,phone,film,award,best</td>\n",
       "      <td>film,award,oscar,england,best</td>\n",
       "      <td>film,game,england,oscar,award</td>\n",
       "      <td>film,award,band,best,company</td>\n",
       "      <td>film,labour,election,party,tax</td>\n",
       "      <td>film,game,injury,player,england</td>\n",
       "      <td>film,award,game,best,labour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>labour,election,blair,tax,game</td>\n",
       "      <td>labour,election,game,film,blair</td>\n",
       "      <td>film,growth,economy,rate,bank</td>\n",
       "      <td>labour,film,brown,england,party</td>\n",
       "      <td>film,rate,mobile,music,game</td>\n",
       "      <td>game,film,music,election,player</td>\n",
       "      <td>film,game,labour,election,blair</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text  \\\n",
       "0           tech  tv future in the hands of viewers with home th...   \n",
       "1       business  worldcom boss  left books alone  former worldc...   \n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3          sport  yeading face newcastle in fa cup premiership s...   \n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                      Method1_LSI                      Method2_LSI  \\\n",
       "0  labour,election,blair,tax,game  labour,election,game,film,blair   \n",
       "1  labour,election,blair,tax,game  labour,election,game,film,blair   \n",
       "2  labour,election,blair,tax,game  labour,election,game,film,blair   \n",
       "3    mobile,phone,film,award,best    film,award,oscar,england,best   \n",
       "4  labour,election,blair,tax,game  labour,election,game,film,blair   \n",
       "\n",
       "                            Method3_LSI                      Method1_LDA  \\\n",
       "0  phone,film,economy,growth,technology    film,game,mobile,music,player   \n",
       "1  phone,film,economy,growth,technology  labour,film,brown,england,party   \n",
       "2         film,game,england,oscar,award    film,game,mobile,music,player   \n",
       "3         film,game,england,oscar,award     film,award,band,best,company   \n",
       "4         film,growth,economy,rate,bank  labour,film,brown,england,party   \n",
       "\n",
       "                            Method2_LDA                      Method3_LDA  \\\n",
       "0           film,rate,mobile,music,game    phone,music,film,company,sale   \n",
       "1  game,mobile,technology,phone,company   film,game,election,phone,party   \n",
       "2        film,labour,election,party,tax  game,film,music,election,player   \n",
       "3        film,labour,election,party,tax  film,game,injury,player,england   \n",
       "4           film,rate,mobile,music,game  game,film,music,election,player   \n",
       "\n",
       "                        Top5_words  \n",
       "0  film,game,music,labour,election  \n",
       "1  game,film,labour,election,phone  \n",
       "2  game,film,election,labour,blair  \n",
       "3      film,award,game,best,labour  \n",
       "4  film,game,labour,election,blair  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating models using Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
